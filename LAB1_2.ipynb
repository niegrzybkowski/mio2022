{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MSE(x, y):\n",
    "    return np.sum((x - y)**2) / len(x)\n",
    "\n",
    "def lin_act(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class Layer:   \n",
    "    def __init__(self, neurons, input_shape, weights, bias, activation):\n",
    "        self.neurons = neurons\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        assert weights.shape == (input_shape[1], neurons)\n",
    "        self.weights = weights\n",
    "        \n",
    "        assert bias.shape == (1, neurons)\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.last_a = None\n",
    "    \n",
    "    \n",
    "    def make_factory(neurons, input_shape, activation, factory):\n",
    "        return Layer(\n",
    "            neurons = neurons,\n",
    "            input_shape = input_shape,\n",
    "            weights = factory((input_shape[1], neurons)),\n",
    "            bias = factory((1, neurons)),\n",
    "            activation = activation\n",
    "        )\n",
    "    \n",
    "    def make_zero(neurons, input_shape, activation):\n",
    "        return Layer.make_factory(neurons, input_shape, activation, np.zeros)\n",
    "    \n",
    "    def make_random(neurons, input_shape, activation):\n",
    "        random_balanced = lambda shape: np.random.random(shape)\n",
    "        return Layer.make_factory(neurons, input_shape, activation, random_balanced)\n",
    "    \n",
    "    def apply(self, inputs):\n",
    "        intensities = inputs @ self.weights\n",
    "        self.last_a = intensities + self.bias\n",
    "        return self.activation(intensities + self.bias)\n",
    "    \n",
    "    def apply_no_intensities(self, inputs):\n",
    "        intensities = inputs @ self.weights\n",
    "        return self.activation(intensities + self.bias)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"LAYER(\\nW:\\n {repr(self.weights)} \\nb:\\n{repr(self.bias)})\\n\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, *layers, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = [*layers]\n",
    "        self.errors = None\n",
    "        self.last_inputs = None\n",
    "        \n",
    "    def get_last_shape(self):\n",
    "        if self.layers:\n",
    "            return self.layers[-1].weights.shape\n",
    "        else:\n",
    "            return self.input_shape\n",
    "    \n",
    "    def add_new_zero_layer(self, neurons, activation=sigmoid):\n",
    "        layer = Layer.make_zero(\n",
    "            neurons,\n",
    "            self.get_last_shape(),\n",
    "            activation\n",
    "        )\n",
    "        self.layers.append(layer)\n",
    "        return layer\n",
    "    \n",
    "    def add_new_random_layer(self, neurons, activation=sigmoid):\n",
    "        layer = Layer.make_random(\n",
    "            neurons,\n",
    "            self.get_last_shape(),\n",
    "            activation\n",
    "        )\n",
    "        self.layers.append(layer)\n",
    "        return layer\n",
    "        \n",
    "    def apply(self, inputs):\n",
    "        self.last_inputs = inputs\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer.apply(x)\n",
    "        return x\n",
    "    \n",
    "    def backpropagate(self, yhat, y, batch_size):\n",
    "        # this shit dont work\n",
    "        errors = [None] * len(self.layers)\n",
    "        errors[-1] = grad_sigmoid(self.layers[-1].last_a) *  (yhat - y)\n",
    "        for i in range(2, len(errors) + 1):\n",
    "            uhm = errors[-i+1] @ np.transpose(self.layers[-i+1].weights)\n",
    "            errors[-i] = grad_sigmoid(self.layers[-i].last_a) * uhm\n",
    "        print(errors)\n",
    "        grad = [None] * len(self.layers)\n",
    "        grad_b = [None] * len(self.layers)\n",
    "        for k in range(len(errors)):\n",
    "            if k == 0:\n",
    "                f_a = self.layers[0].activation(self.last_inputs)\n",
    "            else:\n",
    "                cur_layer = self.layers[k-1]\n",
    "                f_a = cur_layer.activation(cur_layer.last_a)\n",
    "            \n",
    "            grad[k] = np.transpose(f_a) @ errors[k]\n",
    "            grad_b[k] = np.ones((1,batch_size)) @ errors[k] \n",
    "                \n",
    "        for k, layer in enumerate(self.layers):\n",
    "            #print(grad[k].shape)\n",
    "            layer.weights -= 1e-3 * grad[k]\n",
    "            layer.bias -= 1e-3 * grad_b[k]\n",
    "        return grad\n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, nn):\n",
    "        self.nn = nn\n",
    "        \n",
    "    def backpropagate(self, x, y):\n",
    "        yhat = nn.apply(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN(input_shape=(_, 1))\n",
    "hl1 = nn.add_new_random_layer(5)\n",
    "#hl2 = nn.add_new_random_layer(5)\n",
    "ol = nn.add_new_random_layer(1, activation=lin_act)\n",
    "ol.bias = np.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.19973346]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.apply(np.array([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LAYER(\n",
       " W:\n",
       "  array([[0.88859829, 0.98225591, 0.12419479, 0.64327458, 0.88579401]]) \n",
       " b:\n",
       " array([[0.35062152, 0.48505979, 0.72049584, 0.32404472, 0.36443597]])),\n",
       " LAYER(\n",
       " W:\n",
       "  array([[0.21580875],\n",
       "        [0.22740014],\n",
       "        [0.09739063],\n",
       "        [0.21369294],\n",
       "        [0.80355348]]) \n",
       " b:\n",
       " array([[0.]]))]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00133549,  0.00123034,  0.00072755,  0.00151545,  0.00494247],\n",
      "       [-0.0087243 , -0.00707177, -0.00825629, -0.01260723, -0.03227516],\n",
      "       [-0.01108937, -0.00796369, -0.02113974, -0.02121346, -0.04108898]]), array([[ 0.03553656],\n",
      "       [-0.42510541],\n",
      "       [-1.15446926]])]\n",
      "[array([[ 0.00138531,  0.00127596,  0.00075848,  0.00157149,  0.00510148],\n",
      "       [-0.00871855, -0.00706562, -0.00829232, -0.01259484, -0.03209252],\n",
      "       [-0.01109708, -0.00796759, -0.02126085, -0.021221  , -0.0409094 ]]), array([[ 0.03661634],\n",
      "       [-0.42200202],\n",
      "       [-1.1476193 ]])]\n",
      "[array([[ 0.00143497,  0.00132142,  0.00078951,  0.00162734,  0.00525865],\n",
      "       [-0.00871224, -0.00705904, -0.00832691, -0.01258172, -0.03191125],\n",
      "       [-0.01110403, -0.00797098, -0.02137847, -0.02122721, -0.04073092]]), array([[ 0.03767964],\n",
      "       [-0.41893187],\n",
      "       [-1.14083497]])]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    out = nn.apply(np.array([[1], [2], [3]]))\n",
    "    nn.backpropagate(out, np.array([[1], [4], [9]]), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
